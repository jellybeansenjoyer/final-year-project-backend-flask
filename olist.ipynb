{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a30b43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90f100e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3252337d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"olist_customers_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd24eb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Super adooorei o delineador ele é bem preto e eu smp usei o da Mac e serio o da belle angel me surpreendeu pq eu achei que por ser barato iria ser ruim mas NAO é ... o unico problema é q ele é pequeno\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file containing Amazon reviews\n",
    "df = pd.read_csv(\"olist_customers_dataset.csv\")\n",
    "\n",
    "# Define the product ID you want to extract reviews for\n",
    "product_id = \"1e9e8ef04dbcff4541ed26657ea517e5\"\n",
    "\n",
    "# Filter the DataFrame based on the product ID\n",
    "reviews_for_product = df[df['product_id'] == product_id]\n",
    "\n",
    "# Extract the review comment message column from the filtered DataFrame\n",
    "reviews_comment_messages = reviews_for_product['review_comment_message']\n",
    "\n",
    "# Concatenate all review comment messages into a single paragraph\n",
    "paragraph = \" \".join(reviews_comment_messages.dropna())\n",
    "\n",
    "print(paragraph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "509236d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "the JSON object must be str, bytes or bytearray, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m translator \u001b[38;5;241m=\u001b[39m Translator()\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Detect the language of the paragraph\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m language \u001b[38;5;241m=\u001b[39m \u001b[43mtranslator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparagraph\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlang\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# If the detected language is not English, translate the paragraph to English\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m language \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\googletrans\\client.py:369\u001b[0m, in \u001b[0;36mTranslator.detect\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetect\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 369\u001b[0m     translated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    370\u001b[0m     result \u001b[38;5;241m=\u001b[39m Detected(lang\u001b[38;5;241m=\u001b[39mtranslated\u001b[38;5;241m.\u001b[39msrc, confidence\u001b[38;5;241m=\u001b[39mtranslated\u001b[38;5;241m.\u001b[39mextra_data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfidence\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m), response\u001b[38;5;241m=\u001b[39mtranslated\u001b[38;5;241m.\u001b[39m_response)\n\u001b[0;32m    371\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\googletrans\\client.py:219\u001b[0m, in \u001b[0;36mTranslator.translate\u001b[1;34m(self, text, dest, src)\u001b[0m\n\u001b[0;32m    216\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    218\u001b[0m data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(resp)\n\u001b[1;32m--> 219\u001b[0m parsed \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;66;03m# not sure\u001b[39;00m\n\u001b[0;32m    221\u001b[0m should_spacing \u001b[38;5;241m=\u001b[39m parsed[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m3\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\json\\__init__.py:339\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(s, (\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mbytearray\u001b[39m)):\n\u001b[1;32m--> 339\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe JSON object must be str, bytes or bytearray, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    340\u001b[0m                         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n",
      "\u001b[1;31mTypeError\u001b[0m: the JSON object must be str, bytes or bytearray, not NoneType"
     ]
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "#spanish\n",
    "#paragraph=\"Este producto es increíblemente útil en mi día a día. Me encanta lo fácil que es de usar y lo bien que funciona. Definitivamente lo recomendaría a cualquier persona que esté buscando mejorar su vida diaria. ¡Gracias al fabricante por crear un producto tan fantástico\"\n",
    "#french\n",
    "#paragraph=\"C'est produit est incroyablement utile dans ma vie quotidienne. J'adore à quel point il est facile à utiliser et à quel point il fonctionne bien. Je le recommanderais certainement à quiconque cherche à améliorer son quotidien. Merci au fabricant d'avoir créé un produit si fantastique !\"\n",
    "# hindi\n",
    "#paragraph=\"यह उत्कृष्ट उत्पाद है! मुझे यह खरीदने का बहुत ही अच्छा निर्णय था। इसकी गुणवत्ता बहुत अच्छी है और इसका काम भी बहुत ही अच्छा है। मैं इसे सभी को सुझाव दूंगा।\"\n",
    "#kannada\n",
    "#paragraph=\"ಈ ಅದ್ಭುತ ಉತ್ಪನ್ನವಾಗಿದೆ! ಇದನ್ನು ಖರೀದಿಸುವುದು ಒಳ್ಳೆಯ ನಿರ್ಣಯವಾಯಿತು. ಇದರ ಗುಣಮಟ್ಟ ತುಂಬಾ ಚೆನ್ನಾಗಿದೆ ಮತ್ತು ಇದರ ಕೆಲಸವೂ ಅದ್ಭುತವಾಗಿದೆ. ನಾನು ಇದನ್ನು ಎಲ್ಲರಿಗೂ ಶಿಫಾರಸು ಮಾಡುತ್ತೇನೆ.\"\n",
    "\n",
    "# Initialize the translator\n",
    "translator = Translator()\n",
    "\n",
    "# Detect the language of the paragraph\n",
    "language = translator.detect(paragraph).lang\n",
    "\n",
    "# If the detected language is not English, translate the paragraph to English\n",
    "if language != 'en':\n",
    "    translated_paragraph = translator.translate(paragraph, src=language, dest='en').text\n",
    "else:\n",
    "    translated_paragraph = paragraph\n",
    "paragraph = translated_paragraph\n",
    "print(paragraph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acc8d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from string import punctuation\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Sample paragraph\n",
    "#paragraph = \"Your paragraph goes here.\"\n",
    "\n",
    "# Initialize VADER sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Tokenize the paragraph into words\n",
    "words = nltk.word_tokenize(paragraph)\n",
    "\n",
    "# Get English stopwords and punctuation marks\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "english_punctuation = set(punctuation)\n",
    "\n",
    "# Initialize a list to store good, bad, and neutral words\n",
    "good_words = []\n",
    "bad_words = []\n",
    "neutral_words = []\n",
    "\n",
    "# Function to check if a word is meaningful\n",
    "def is_meaningful(word):\n",
    "    synsets = wordnet.synsets(word)\n",
    "    return len(synsets) > 0\n",
    "\n",
    "# Classify words as good, bad, or neutral\n",
    "for word in words:\n",
    "    if word.lower() in english_stopwords or word in english_punctuation:\n",
    "        continue\n",
    "    if not is_meaningful(word):\n",
    "        continue\n",
    "    \n",
    "    score = sia.polarity_scores(word)\n",
    "    if score['compound'] >= 0.05:\n",
    "        good_words.append(word)\n",
    "    elif score['compound'] <= -0.05:\n",
    "        bad_words.append(word)\n",
    "    else:\n",
    "        neutral_words.append(word)\n",
    "\n",
    "# Convert lists to sets to remove duplicates\n",
    "good_words = set(good_words)\n",
    "bad_words = set(bad_words)\n",
    "neutral_words = set(neutral_words)\n",
    "\n",
    "print(\"Good Words:\", good_words)\n",
    "print(\"Bad Words:\", bad_words)\n",
    "print(\"Neutral Words:\", neutral_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfbb6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "labels = ['Good', 'Bad', 'Neutral']\n",
    "counts = [len(good_words), len(bad_words), len(neutral_words)]\n",
    "\n",
    "plt.bar(labels, counts, color=['green', 'red', 'blue'])\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Good, Bad, and Neutral Words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2f2dcb4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'good_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m weight_neutral \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Calculate sentiment score based on weighted counts\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m sentiment_score \u001b[38;5;241m=\u001b[39m (weight_good \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mgood_words\u001b[49m)) \u001b[38;5;241m+\u001b[39m (weight_bad \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(bad_words)) \u001b[38;5;241m+\u001b[39m (weight_neutral \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(neutral_words))\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSentiment Score:\u001b[39m\u001b[38;5;124m\"\u001b[39m, sentiment_score)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'good_words' is not defined"
     ]
    }
   ],
   "source": [
    "# Assign weights to good, bad, and neutral words\n",
    "weight_good = 2\n",
    "weight_bad = -2\n",
    "weight_neutral = 1\n",
    "\n",
    "# Calculate sentiment score based on weighted counts\n",
    "sentiment_score = (weight_good * len(good_words)) + (weight_bad * len(bad_words)) + (weight_neutral * len(neutral_words))\n",
    "\n",
    "print(\"Sentiment Score:\", sentiment_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "189ab23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Score: 1.2142857142857144\n"
     ]
    }
   ],
   "source": [
    "# Sample weights (you can adjust these according to your preference)\n",
    "weight_good = 2\n",
    "weight_bad = -2\n",
    "weight_neutral = 1\n",
    "\n",
    "# Calculate the sentiment score\n",
    "total_words = len(good_words) + len(bad_words) + len(neutral_words)\n",
    "sentiment_score = (len(good_words) * weight_good + len(bad_words) * weight_bad + len(neutral_words) * weight_neutral) / total_words\n",
    "\n",
    "# Normalize the sentiment score to ensure it lies between 0 and 1\n",
    "normalized_sentiment_score = (sentiment_score + 1) / 2\n",
    "\n",
    "print(\"Sentiment Score:\", normalized_sentiment_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc312c91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
